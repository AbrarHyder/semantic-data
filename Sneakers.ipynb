{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./opt/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: click in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "def normalization(text):\n",
    "    p_text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)\n",
    "    p_text = \" \".join(p_text.split())\n",
    "#     p_text.lower()\n",
    "    return p_text\n",
    "\n",
    "def Lemmatization(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    required_words = [wordnet_lemmatizer.lemmatize(x, 'v') for x in tokens]\n",
    "    # sentence = \" \".join(required_words)\n",
    "    return required_words\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "#     lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "    return lemmatized_words\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    final_query = set(words)-set(stop_words)\n",
    "    return list(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def create_pos_tag(sent):\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter query: Give 4 sneakers whose color is white\n",
      "Give 4 sneakers whose color is white\n",
      "['Give', '4', 'sneaker', 'whose', 'color', 'is', 'white']\n",
      "[('Give', 'VB'), ('4', 'CD'), ('sneaker', 'NN'), ('whose', 'WP$'), ('color', 'NN'), ('is', 'VBZ'), ('white', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "query = input(\"enter query: \") # Give 4 sneakers whose color is white\n",
    "\n",
    "nor_query = normalization(query)\n",
    "print(nor_query)\n",
    "lem_query = lemmatize_sentence(nor_query)\n",
    "print(lem_query)\n",
    "# final_query = remove_stop_words(lem_query)\n",
    "tags = create_pos_tag(lem_query)\n",
    "\n",
    "# print(final_query)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ['sneaker', 'color'] ['white']\n"
     ]
    }
   ],
   "source": [
    "# classes = [\"Sneakers\", \"brands\", \"colaboration\" ]\n",
    "# data_prop = [\"color\", \"model_name\"]\n",
    "\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "# classes = []\n",
    "\n",
    "# named_entity = nltk.ne_chunk(tags)\n",
    "\n",
    "# print(tags)\n",
    "# print(named_entity)\n",
    "\n",
    "# for entity in named_entity:\n",
    "#     if hasattr(entity, 'label'):\n",
    "# #         print(entity.label())\n",
    "#         name = '_'.join(c[0] for c in entity)\n",
    "#         print(name)\n",
    "#         classes.append(name)\n",
    "\n",
    "sub_cla = \"_\".join([x[0] for x in tags if x[1] == \"NNP\" ])\n",
    "cla = [x[0] for x in tags if x[1] == \"NN\" ]\n",
    "data_prop = [x[0] for x in tags if x[1] == \"JJ\"]\n",
    "print(sub_cla, cla, data_prop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query1 Give 4 sneakers whose color is white\n",
    "from SPARQLWrapper import SPARQLWrapper2\n",
    "sparql = SPARQLWrapper2(\"http://localhost:3030/snickers/query\")\n",
    "sparql.setQuery(f\"\"\"\n",
    "                PREFIX sn: <http://www.sneaker_head.com/sneaker_head#>\n",
    "                SELECT ?model_name ?price ?color\n",
    "                WHERE {{\n",
    "                  ?x a sn:{sub_cla}.\n",
    "                  ?x sn:Model ?model_name.\n",
    "                  ?x sn:price ?price.\n",
    "                  ?x sn:color ?color. \n",
    "                  FILTER (CONTAINS(?color, \"{data_prop[0].capitalize()}\"))\n",
    "                  }}\n",
    "                  ORDER BY ASC(?price)\n",
    "                \"\"\")\n",
    "data = sparql.query().bindings\n",
    "\n",
    "for x in data:\n",
    "    print(f\"{x['model_name'].value}\\t {x['price'].value}\\t {x['color'].value} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter query: Give 4 sneakers whose color is white\n",
      "Give 4 sneakers whose color is white\n",
      "['Give', '4', 'sneaker', 'whose', 'color', 'is', 'white']\n",
      "[('Give', 'VB'), ('4', 'CD'), ('sneaker', 'NN'), ('whose', 'WP$'), ('color', 'NN'), ('is', 'VBZ'), ('white', 'JJ')]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ['sneaker', 'color'] ['white']\n"
     ]
    }
   ],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper2\n",
    "query = input(\"enter query: \") # Give 4 sneakers whose color is white\n",
    "\n",
    "nor_query = normalization(query)\n",
    "print(nor_query)\n",
    "lem_query = lemmatize_sentence(nor_query)\n",
    "print(lem_query)\n",
    "# final_query = remove_stop_words(lem_query)\n",
    "tags = create_pos_tag(lem_query)\n",
    "\n",
    "# print(final_query)\n",
    "print(tags)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "sub_cla = \"_\".join([x[0] for x in tags if x[1] == \"NNP\" ])\n",
    "cla = [x[0] for x in tags if x[1] == \"NN\" ]\n",
    "data_prop = [x[0] for x in tags if x[1] == \"JJ\"]\n",
    "print(sub_cla, cla, data_prop)\n",
    "\n",
    "sparql = SPARQLWrapper2(\"http://localhost:3030/snickers/query\")\n",
    "sparql.setQuery(f\"\"\"\n",
    "                PREFIX sn: <http://www.sneaker_head.com/sneaker_head#>\n",
    "                SELECT ?model_name ?price ?color\n",
    "                WHERE {{\n",
    "                  ?x a sn:{sub_cla}.\n",
    "                  ?x sn:Model ?model_name.\n",
    "                  ?x sn:price ?price.\n",
    "                  ?x sn:color ?color. \n",
    "                  FILTER (CONTAINS(?color, \"{data_prop[0].capitalize()}\"))\n",
    "                  }}\n",
    "                  ORDER BY ASC(?price)\n",
    "                \"\"\")\n",
    "data = sparql.query().bindings\n",
    "\n",
    "for x in data:\n",
    "    print(f\"{x['model_name'].value}\\t {x['price'].value}\\t {x['color'].value} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
